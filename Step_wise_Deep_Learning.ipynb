{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Create VRP"
      ],
      "metadata": {
        "id": "r70TX6HEr0nQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0IYQk-UPntUg"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from torch_geometric.data import Data\n",
        "from torch_geometric.loader import DataLoader\n",
        "from  tqdm import tqdm\n",
        "def creat_instance(num,n_nodes=100,random_seed=None):\n",
        "    if random_seed is None:\n",
        "        random_seed = np.random.randint(123456789)\n",
        "    np.random.seed(random_seed)\n",
        "    def random_tsp(n_nodes,random_seed=None):\n",
        "\n",
        "        data = np.random.uniform(0,1,(n_nodes,2))\n",
        "        return data\n",
        "    datas = random_tsp(n_nodes)\n",
        "\n",
        "    def c_dist(x1,x2):\n",
        "        return ((x1[0]-x2[0])**2+(x1[1]-x2[1])**2)**0.5\n",
        "    #edges = torch.zeros(n_nodes,n_nodes)\n",
        "    edges = np.zeros((n_nodes,n_nodes,1))\n",
        "\n",
        "    for i, (x1, y1) in enumerate(datas):\n",
        "        for j, (x2, y2) in enumerate(datas):\n",
        "            d = c_dist((x1, y1), (x2, y2))\n",
        "            edges[i][j][0]=d\n",
        "    edges = edges.reshape(-1, 1)\n",
        "    CAPACITIES = {\n",
        "        10: 2.,\n",
        "        20: 3.,\n",
        "        50: 4.,\n",
        "        100: 5.\n",
        "    }\n",
        "\n",
        "    demand = np.random.randint(1, 10, size=(n_nodes-1)) # Demand, uniform integer 1 ... 9\n",
        "    demand = np.array(demand)/10\n",
        "    demand = np.insert(demand,0,0.)\n",
        "    capcity = CAPACITIES[n_nodes-1]\n",
        "    return datas,edges,demand,capcity#demand(num,node) capcity(num)\n",
        "\n",
        "'''a,s,d,f = creat_instance(2,21)\n",
        "print(d,f)'''\n",
        "def creat_data(n_nodes,num_samples=10000 ,batch_size=32):\n",
        "    edges_index = []\n",
        "    for i in range(n_nodes):\n",
        "        for j in range(n_nodes):\n",
        "            edges_index.append([i, j])\n",
        "    edges_index = torch.LongTensor(edges_index)\n",
        "    edges_index = edges_index.transpose(dim0=0,dim1=1)\n",
        "\n",
        "    datas = []\n",
        "\n",
        "    for i in range(num_samples):\n",
        "        node, edge, demand, capcity = creat_instance(num_samples, n_nodes)\n",
        "        data = Data(x=torch.from_numpy(node).float(), edge_index=edges_index,edge_attr=torch.from_numpy(edge).float(),\n",
        "                    demand=torch.tensor(demand).unsqueeze(-1).float(),capcity=torch.tensor(capcity).unsqueeze(-1).float())\n",
        "        datas.append(data)\n",
        "    #print(datas)\n",
        "    dl = DataLoader(datas, batch_size=batch_size)\n",
        "    return dl\n",
        "\n",
        "def reward(static, tour_indices,n_nodes,batch_size):\n",
        "\n",
        "    static = static.reshape(-1,n_nodes,2)\n",
        "    #print(static.shape)\n",
        "    static = static.transpose(2,1)\n",
        "    tour_indices = tour_indices.reshape(batch_size,-1)\n",
        "    idx = tour_indices.unsqueeze(1).expand(-1,static.size(1),-1)\n",
        "    tour = torch.gather(static.data, 2, idx).permute(0, 2, 1)\n",
        "    #print(tour.shape)\n",
        "    #print(idx.shape)\n",
        "    y = torch.cat((tour, tour[:, :1]), dim=1)\n",
        "\n",
        "    tour_len = torch.sqrt(torch.sum(torch.pow(y[:, :-1] - y[:, 1:], 2), dim=2))\n",
        "    #print(tour_len.sum(1))\n",
        "    return tour_len.sum(1).detach()\n",
        "\n",
        "\n",
        "def reward1(static, tour_indices,n_nodes):\n",
        "\n",
        "    static = static.reshape(-1,n_nodes,2)\n",
        "\n",
        "    static = static.transpose(2,1)\n",
        "\n",
        "    idx = tour_indices.unsqueeze(1).expand(-1,static.size(1),-1)\n",
        "\n",
        "    tour = torch.gather(static, 2, idx).permute(0, 2, 1)\n",
        "    #print(tour.shape,tour[0])\n",
        "    #print(idx.shape,idx[0])\n",
        "    # Make a full tour by returning to the start\n",
        "    start = static.data[:, :, 0].unsqueeze(1)\n",
        "    y = torch.cat((start, tour,start), dim=1)\n",
        "\n",
        "    # Euclidean distance between each consecutive point\n",
        "    tour_len = torch.sqrt(torch.sum(torch.pow(y[:, :-1] - y[:, 1:], 2), dim=2))\n",
        "    #print(tour_len.sum(1))\n",
        "    return tour_len.sum(1).detach()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# VRP Update"
      ],
      "metadata": {
        "id": "8Lr2KSfWsBJr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import time\n",
        "\n",
        "def update_state(demand,dynamic_capcity,selected,c=20):#dynamic_capcity(num,1)\n",
        "\n",
        "    depot  =  selected.squeeze(-1).eq(0)#Is there a group to access the depot\n",
        "\n",
        "    current_demand = torch.gather(demand,1,selected)\n",
        "\n",
        "    dynamic_capcity = dynamic_capcity-current_demand\n",
        "    if depot.any():\n",
        "        dynamic_capcity[depot.nonzero().squeeze()] = c\n",
        "\n",
        "    return dynamic_capcity.detach()#(bach_size,1)\n",
        "\n",
        "\n",
        "def update_mask(demand,capcity,selected,mask,i):\n",
        "    go_depot = selected.squeeze(-1).eq(0)#If there is a route to select a depot, mask the depot, otherwise it will not mask the depot\n",
        "    #print(go_depot.nonzero().squeeze())\n",
        "    #visit = selected.ne(0)\n",
        "\n",
        "    mask1 = mask.scatter(1, selected.expand(mask.size(0), -1), 1)\n",
        "\n",
        "    if (~go_depot).any():\n",
        "        mask1[(~go_depot).nonzero(),0] = 0\n",
        "\n",
        "    if i+1>demand.size(1):\n",
        "        is_done = (mask1[:, 1:].sum(1) >= (demand.size(1) - 1)).float()\n",
        "        combined = is_done.gt(0)\n",
        "        mask1[combined.nonzero(), 0] = 0\n",
        "        '''for i in range(demand.size(0)):\n",
        "            if not mask1[i,1:].eq(0).any():\n",
        "                mask1[i,0] = 0'''\n",
        "    a = demand>capcity\n",
        "    mask = a + mask1\n",
        "\n",
        "    return mask.detach(),mask1.detach()"
      ],
      "metadata": {
        "id": "rTuFSQRdsHKv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Rolout Baseline"
      ],
      "metadata": {
        "id": "Zu5Bzm38sIGk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "from scipy.stats import ttest_rel\n",
        "import copy\n",
        "\n",
        "\n",
        "from torch.nn import DataParallel\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "def get_inner_model(model):\n",
        "    return model.module if isinstance(model, DataParallel) else model\n",
        "def rollout1(model, dataset, n_nodes):\n",
        "\n",
        "    model.eval()\n",
        "    def eval_model_bat(bat):\n",
        "        with torch.no_grad():\n",
        "            cost, _= model(bat,n_nodes*2,True)\n",
        "            cost = reward1(bat.x,cost.detach(), n_nodes)\n",
        "        return cost.cpu()\n",
        "    totall_cost = torch.cat([eval_model_bat(bat.to(device))for bat in dataset], 0)\n",
        "    return totall_cost\n",
        "\n",
        "class RolloutBaseline():\n",
        "\n",
        "    def __init__(self, model,  dataset, n_nodes=50,epoch=0):\n",
        "        super(RolloutBaseline, self).__init__()\n",
        "        self.n_nodes = n_nodes\n",
        "        self.dataset = dataset\n",
        "        self._update_model(model, epoch)\n",
        "    def _update_model(self, model, epoch, dataset=None):\n",
        "        self.model = copy.deepcopy(model)\n",
        "        self.bl_vals = rollout1(self.model, self.dataset, n_nodes=self.n_nodes).cpu().numpy()\n",
        "        self.mean = self.bl_vals.mean()\n",
        "        self.epoch = epoch\n",
        "\n",
        "    def eval(self, x, n_nodes):\n",
        "\n",
        "        with torch.no_grad():\n",
        "            tour, _ = self.model(x,n_nodes,True)\n",
        "            v= reward1(x.x, tour.detach(), n_nodes)\n",
        "\n",
        "        # There is no loss\n",
        "        return v\n",
        "\n",
        "    def epoch_callback(self, model, epoch):\n",
        "\n",
        "        print(\"Evaluating candidate model on evaluation dataset\")\n",
        "        candidate_vals = rollout1(model, self.dataset, self.n_nodes).cpu().numpy()\n",
        "\n",
        "        candidate_mean = candidate_vals.mean()\n",
        "\n",
        "        print(\"Epoch {} candidate mean {}, baseline epoch {} mean {}, difference {}\".format(\n",
        "            epoch, candidate_mean, self.epoch, self.mean, candidate_mean - self.mean))\n",
        "        if candidate_mean - self.mean < 0:\n",
        "            # Calc p value\n",
        "            t, p = ttest_rel(candidate_vals, self.bl_vals)\n",
        "\n",
        "            p_val = p / 2  # one-sided\n",
        "            assert t < 0, \"T-statistic should be negative\"\n",
        "            print(\"p-value: {}\".format(p_val))\n",
        "            if p_val < 0.05:\n",
        "                print('Update baseline')\n",
        "                self._update_model(model, epoch)\n",
        "\n",
        "    def state_dict(self):\n",
        "        return {\n",
        "            'model': self.model,\n",
        "            'dataset': self.dataset,\n",
        "            'epoch': self.epoch\n",
        "        }\n",
        "\n",
        "    def load_state_dict(self, state_dict):\n",
        "        load_model = copy.deepcopy(self.model)\n",
        "        get_inner_model(load_model).load_state_dict(get_inner_model(state_dict['model']).state_dict())\n",
        "        self._update_model(load_model, state_dict['epoch'], state_dict['dataset'])"
      ],
      "metadata": {
        "id": "LzlWd5FesSyU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Actor"
      ],
      "metadata": {
        "id": "Hfc4xGbYsdoP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import MessagePassing\n",
        "from torch_geometric.utils import remove_self_loops, add_self_loops, softmax\n",
        "import math\n",
        "from torch.distributions.categorical import Categorical\n",
        "INIT = False\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "#device = torch.device('cpu')\n",
        "max_grad_norm = 2\n",
        "n_nodes=21\n",
        "\n",
        "\n",
        "# device = torch.device('cpu')\n",
        "\n",
        "\n",
        "class GatConv(MessagePassing):\n",
        "    def __init__(self, in_channels, out_channels, edge_channels,\n",
        "                 negative_slope=0.2, dropout=0):\n",
        "        super(GatConv, self).__init__(aggr='add')\n",
        "\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.negative_slope = negative_slope\n",
        "        self.dropout = dropout\n",
        "\n",
        "        self.fc = nn.Linear(in_channels, out_channels)\n",
        "        self.attn = nn.Linear(2 * out_channels + edge_channels, out_channels)\n",
        "        if INIT:\n",
        "            for name, p in self.named_parameters():\n",
        "                if 'weight' in name:\n",
        "                    if len(p.size()) >= 2:\n",
        "                        nn.init.orthogonal_(p, gain=1)\n",
        "                elif 'bias' in name:\n",
        "                    nn.init.constant_(p, 0)\n",
        "\n",
        "    def forward(self, x, edge_index, edge_attr, size=None):\n",
        "        x = self.fc(x)\n",
        "        return self.propagate(edge_index, size=size, x=x, edge_attr=edge_attr)\n",
        "\n",
        "    def message(self, edge_index_i, x_i, x_j, size_i, edge_attr):\n",
        "        x = torch.cat([x_i, x_j, edge_attr], dim=-1)\n",
        "        alpha = self.attn(x)\n",
        "        alpha = F.leaky_relu(alpha, self.negative_slope)\n",
        "        alpha = softmax(alpha, edge_index_i, size_i)\n",
        "\n",
        "        # Sample attention coefficients stochastically.\n",
        "        alpha = F.dropout(alpha, p=self.dropout, training=self.training)\n",
        "\n",
        "        return x_j * alpha\n",
        "\n",
        "    def update(self, aggr_out):\n",
        "        return aggr_out\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_node_dim, hidden_node_dim, input_edge_dim, hidden_edge_dim, conv_layers=3, n_heads=4):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.hidden_node_dim = hidden_node_dim\n",
        "        self.fc_node = nn.Linear(input_node_dim, hidden_node_dim)\n",
        "        self.bn = nn.BatchNorm1d(hidden_node_dim)\n",
        "        self.be = nn.BatchNorm1d(hidden_edge_dim)\n",
        "        self.fc_edge = nn.Linear(input_edge_dim, hidden_edge_dim)  # 1-16\n",
        "\n",
        "        self.convs1 = nn.ModuleList(\n",
        "            [GatConv(hidden_node_dim, hidden_node_dim, hidden_edge_dim) for i in range(conv_layers)])\n",
        "        if INIT:\n",
        "            for name, p in self.named_parameters():\n",
        "                if 'weight' in name:\n",
        "                    if len(p.size()) >= 2:\n",
        "                        nn.init.orthogonal_(p, gain=1)\n",
        "                elif 'bias' in name:\n",
        "                    nn.init.constant_(p, 0)\n",
        "\n",
        "    def forward(self, data):\n",
        "        batch_size = data.num_graphs\n",
        "        # print(batch_size)\n",
        "        # edge_attr = data.edge_attr\n",
        "\n",
        "        x = torch.cat([data.x, data.demand], -1)\n",
        "        x = self.fc_node(x)\n",
        "        x = self.bn(x)\n",
        "        edge_attr = self.fc_edge(data.edge_attr)\n",
        "        edge_attr = self.be(edge_attr)\n",
        "        for conv in self.convs1:\n",
        "            # x = conv(x,data.edge_index)\n",
        "            x1 = conv(x, data.edge_index, edge_attr)\n",
        "            x = x + x1\n",
        "\n",
        "        x = x.reshape((batch_size, -1, self.hidden_node_dim))\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "class Attention1(nn.Module):\n",
        "    def __init__(self, n_heads, cat, input_dim, hidden_dim, attn_dropout=0.1, dropout=0):\n",
        "        super(Attention1, self).__init__()\n",
        "\n",
        "        self.n_heads = n_heads\n",
        "        self.input_dim = input_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.head_dim = self.hidden_dim / self.n_heads\n",
        "        self.norm = 1 / math.sqrt(self.head_dim)\n",
        "\n",
        "        self.w = nn.Linear(input_dim * cat, hidden_dim, bias=False)\n",
        "        self.k = nn.Linear(input_dim, hidden_dim, bias=False)\n",
        "        self.v = nn.Linear(input_dim, hidden_dim, bias=False)\n",
        "        self.fc = nn.Linear(hidden_dim, hidden_dim, bias=False)\n",
        "        if INIT:\n",
        "            for name, p in self.named_parameters():\n",
        "                if 'weight' in name:\n",
        "                    if len(p.size()) >= 2:\n",
        "                        nn.init.orthogonal_(p, gain=1)\n",
        "                elif 'bias' in name:\n",
        "                    nn.init.constant_(p, 0)\n",
        "\n",
        "    def forward(self, state_t, context, mask):\n",
        "        '''\n",
        "        :param state_t: (batch_size,1,input_dim*3(GATembeding,fist_node,end_node))\n",
        "        :param context: （batch_size,n_nodes,input_dim）\n",
        "        :param mask: selected nodes  (batch_size,n_nodes)\n",
        "        :return:\n",
        "        '''\n",
        "        batch_size, n_nodes, input_dim = context.size()\n",
        "        Q = self.w(state_t).view(batch_size, 1, self.n_heads, -1)\n",
        "        K = self.k(context).view(batch_size, n_nodes, self.n_heads, -1)\n",
        "        V = self.v(context).view(batch_size, n_nodes, self.n_heads, -1)\n",
        "        Q, K, V = Q.transpose(1, 2), K.transpose(1, 2), V.transpose(1, 2)\n",
        "\n",
        "        compatibility = self.norm * torch.matmul(Q, K.transpose(2,3))  # (batch_size,n_heads,1,hidden_dim)*(batch_size,n_heads,hidden_dim,n_nodes)\n",
        "        compatibility = compatibility.squeeze(2)  # (batch_size,n_heads,n_nodes)\n",
        "        mask = mask.unsqueeze(1).expand_as(compatibility)\n",
        "        u_i = compatibility.masked_fill(mask.bool(), float(\"-inf\"))\n",
        "\n",
        "        scores = F.softmax(u_i, dim=-1)  # (batch_size,n_heads,n_nodes)\n",
        "        scores = scores.unsqueeze(2)\n",
        "        out_put = torch.matmul(scores, V)  # (batch_size,n_heads,1,n_nodes )*(batch_size,n_heads,n_nodes,head_dim)\n",
        "        out_put = out_put.squeeze(2).view(batch_size, self.hidden_dim)  # （batch_size,n_heads,hidden_dim）\n",
        "        out_put = self.fc(out_put)\n",
        "\n",
        "        return out_put  # (batch_size,hidden_dim)\n",
        "\n",
        "\n",
        "class ProbAttention(nn.Module):\n",
        "    def __init__(self, n_heads, input_dim, hidden_dim):\n",
        "        super(ProbAttention, self).__init__()\n",
        "        self.n_heads = n_heads\n",
        "        self.input_dim = input_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "\n",
        "        self.norm = 1 / math.sqrt(hidden_dim)\n",
        "        self.k = nn.Linear(input_dim, hidden_dim, bias=False)\n",
        "        self.mhalayer = Attention1(n_heads, 1, input_dim, hidden_dim)\n",
        "        if INIT:\n",
        "            for name, p in self.named_parameters():\n",
        "                if 'weight' in name:\n",
        "                    if len(p.size()) >= 2:\n",
        "                        nn.init.orthogonal_(p, gain=1)\n",
        "                elif 'bias' in name:\n",
        "                    nn.init.constant_(p, 0)\n",
        "\n",
        "    def forward(self, state_t, context, mask,T):\n",
        "        '''\n",
        "        :param state_t: (batch_size,1,input_dim*3(GATembeding,fist_node,end_node))\n",
        "        :param context: （batch_size,n_nodes,input_dim）\n",
        "        :param mask: selected nodes  (batch_size,n_nodes)\n",
        "        :return:softmax_score\n",
        "        '''\n",
        "        x = self.mhalayer(state_t, context, mask)\n",
        "\n",
        "        batch_size, n_nodes, input_dim = context.size()\n",
        "        Q = x.view(batch_size, 1, -1)\n",
        "        K = self.k(context).view(batch_size, n_nodes, -1)\n",
        "        compatibility = self.norm * torch.matmul(Q, K.transpose(1, 2))  # (batch_size,1,n_nodes)\n",
        "        compatibility = compatibility.squeeze(1)\n",
        "        x = torch.tanh(compatibility)\n",
        "        x = x * (10)\n",
        "        x = x.masked_fill(mask.bool(), float(\"-inf\"))\n",
        "        scores = F.softmax(x/T, dim=-1)\n",
        "        return scores\n",
        "\n",
        "class Decoder1(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim):\n",
        "        super(Decoder1, self).__init__()\n",
        "\n",
        "        super(Decoder1, self).__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "\n",
        "        self.prob = ProbAttention(8, input_dim, hidden_dim)\n",
        "\n",
        "        self.fc = nn.Linear(hidden_dim+1, hidden_dim, bias=False)\n",
        "        self.fc1 = nn.Linear(hidden_dim, hidden_dim, bias=False)\n",
        "\n",
        "        #self._input = nn.Parameter(torch.Tensor(2 * hidden_dim))\n",
        "        #self._input.data.uniform_(-1, 1)\n",
        "        if INIT:\n",
        "            for name, p in self.named_parameters():\n",
        "                if 'weight' in name:\n",
        "                    if len(p.size()) >= 2:\n",
        "                        nn.init.orthogonal_(p, gain=1)\n",
        "                elif 'bias' in name:\n",
        "                    nn.init.constant_(p, 0)\n",
        "\n",
        "    def forward(self, encoder_inputs, pool,capcity,demand, n_steps,T, greedy=False):\n",
        "\n",
        "        mask1 = encoder_inputs.new_zeros((encoder_inputs.size(0), encoder_inputs.size(1)))\n",
        "        mask = encoder_inputs.new_zeros((encoder_inputs.size(0), encoder_inputs.size(1)))\n",
        "\n",
        "        dynamic_capcity = capcity.view(encoder_inputs.size(0),-1)#bat_size\n",
        "        demands = demand.view(encoder_inputs.size(0),encoder_inputs.size(1))#（batch_size,seq_len）\n",
        "        index = torch.zeros(encoder_inputs.size(0)).to(device).long()\n",
        "\n",
        "        log_ps = []\n",
        "        actions = []\n",
        "\n",
        "        for i in range(n_steps):\n",
        "            if not mask1[:, 1:].eq(0).any():\n",
        "                break\n",
        "            if i == 0:\n",
        "                _input = encoder_inputs[:, 0, :]  # depot\n",
        "\n",
        "            # -----------------------------------------------------------------------------pool+cat(first_node,current_node)\n",
        "            decoder_input = torch.cat([_input, dynamic_capcity], -1)\n",
        "            decoder_input = self.fc(decoder_input)\n",
        "            pool = self.fc1(pool)\n",
        "            decoder_input = decoder_input + pool\n",
        "            # -----------------------------------------------------------------------------cat(pool,first_node,current_node)\n",
        "            '''decoder_input = torch.cat([pool,_input,dynamic_capcity], dim=-1)\n",
        "            decoder_input  = self.fc(decoder_input)'''\n",
        "            # -----------------------------------------------------------------------------------------------------------\n",
        "            if i == 0:\n",
        "                mask, mask1 = update_mask(demands, dynamic_capcity, index.unsqueeze(-1), mask1, i)\n",
        "            p = self.prob(decoder_input, encoder_inputs, mask,T)\n",
        "            dist = Categorical(p)\n",
        "            if greedy:\n",
        "                _, index = p.max(dim=-1)\n",
        "            else:\n",
        "                index = dist.sample()\n",
        "\n",
        "            actions.append(index.data.unsqueeze(1))\n",
        "            log_p = dist.log_prob(index)\n",
        "            is_done = (mask1[:, 1:].sum(1) >= (encoder_inputs.size(1) - 1)).float()\n",
        "            log_p = log_p * (1. - is_done)\n",
        "\n",
        "            log_ps.append(log_p.unsqueeze(1))\n",
        "\n",
        "            dynamic_capcity = update_state(demands, dynamic_capcity, index.unsqueeze(-1), capcity[0].item())\n",
        "            mask, mask1 = update_mask(demands, dynamic_capcity, index.unsqueeze(-1), mask1, i)\n",
        "\n",
        "            _input = torch.gather(encoder_inputs, 1,\n",
        "                                  index.unsqueeze(-1).unsqueeze(-1).expand(encoder_inputs.size(0), -1,\n",
        "                                                                           encoder_inputs.size(2))).squeeze(1)\n",
        "        log_ps = torch.cat(log_ps, dim=1)\n",
        "        actions = torch.cat(actions, dim=1)\n",
        "\n",
        "        log_p = log_ps.sum(dim=1)\n",
        "\n",
        "        return actions, log_p\n",
        "\n",
        "class Model(nn.Module):\n",
        "    def __init__(self, input_node_dim, hidden_node_dim, input_edge_dim, hidden_edge_dim, conv_laysers):\n",
        "        super(Model, self).__init__()\n",
        "        self.encoder = Encoder(input_node_dim, hidden_node_dim, input_edge_dim, hidden_edge_dim, conv_laysers)\n",
        "        self.decoder = Decoder1(hidden_node_dim, hidden_node_dim)\n",
        "\n",
        "    def forward(self, datas,  n_steps,greedy=False,T=1):\n",
        "        x = self.encoder(datas)  # (batch,seq_len,hidden_node_dim)\n",
        "        pooled = x.mean(dim=1)\n",
        "        demand = datas.demand\n",
        "        capcity = datas.capcity\n",
        "\n",
        "        actions, log_p = self.decoder(x, pooled, capcity,demand, n_steps,T, greedy)\n",
        "        return actions, log_p"
      ],
      "metadata": {
        "id": "GN6P5drysdXX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train"
      ],
      "metadata": {
        "id": "0ulzvyvbsuQT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import datetime\n",
        "import numpy as np\n",
        "import torch\n",
        "import os\n",
        "import time\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "from collections import OrderedDict\n",
        "from collections import namedtuple\n",
        "from itertools import product\n",
        "from torch.optim.lr_scheduler import LambdaLR\n",
        "\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "#device = torch.device('cpu')\n",
        "n_nodes = 21\n",
        "print(device)\n",
        "steps = n_nodes\n",
        "def rollout(model, dataset,batch_size, n_nodes):\n",
        "\n",
        "    model.eval()\n",
        "    def eval_model_bat(bat):\n",
        "        with torch.no_grad():\n",
        "            cost, _ = model(bat,n_nodes*2,True)\n",
        "\n",
        "            cost = reward1(bat.x,cost.detach(), n_nodes)\n",
        "        return cost.cpu()\n",
        "    totall_cost = torch.cat([eval_model_bat(bat.to(device))for bat in dataset], 0)\n",
        "    return totall_cost\n",
        "\n",
        "max_grad_norm = 2\n",
        "\n",
        "rewardss = []\n",
        "def adv_normalize(adv):\n",
        "    std = adv.std()\n",
        "    assert std != 0. and not torch.isnan(std), 'Need nonzero std'\n",
        "    n_advs = (adv - adv.mean()) / (adv.std() + 1e-8)\n",
        "    return n_advs\n",
        "\n",
        "def train():\n",
        "    #------------------------------------------------------------------------------------------------------------------------------\n",
        "    class RunBuilder():\n",
        "        @staticmethod\n",
        "        def get_runs(params):\n",
        "            Run = namedtuple('Run', params.keys())\n",
        "            runs = []\n",
        "            for v in product(*params.values()):\n",
        "                runs.append(Run(*v))\n",
        "            return runs\n",
        "\n",
        "    params = OrderedDict(\n",
        "        lr=[1e-3],\n",
        "        batch_size=[512],\n",
        "        hidden_node_dim=[128],\n",
        "        hidden_edge_dim=[16],\n",
        "        conv_laysers=[4],\n",
        "        data_size=[168000]\n",
        "    )\n",
        "    runs = RunBuilder.get_runs(params)\n",
        "    #-------------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "    folder = 'Vrp-{}-GAT'.format(n_nodes)\n",
        "    filename = 'rollout'\n",
        "    for lr,batch_size,hidden_node_dim,hidden_edge_dim,conv_laysers,data_size in runs:\n",
        "        print('lr','batch_size','hidden_node_dim','hidden_edge_dim','conv_laysers:',lr,batch_size,hidden_node_dim,hidden_edge_dim,conv_laysers)\n",
        "        data_loder = creat_data(n_nodes, data_size,batch_size=batch_size)\n",
        "        valid_loder = creat_data(n_nodes, 10000, batch_size=batch_size)\n",
        "        print('Data creation completed')\n",
        "\n",
        "        actor = Model(3, hidden_node_dim, 1, hidden_edge_dim, conv_laysers=conv_laysers).to(device)\n",
        "        rol_baseline = RolloutBaseline(actor,valid_loder,n_nodes=steps)\n",
        "        #initWeights(actor)\n",
        "        filepath = os.path.join(folder, filename)\n",
        "        '''path = os.path.join(filepath,'%s' % 3)\n",
        "                if os.path.exists(path):\n",
        "                    path1 = os.path.join(path, 'actor.pt')\n",
        "                    self.agent.old_polic.load_state_dict(torch.load(path1, device))'''\n",
        "        actor_optim = optim.Adam(actor.parameters(), lr=lr)\n",
        "\n",
        "        costs = []\n",
        "        for epoch in range(100):\n",
        "            print(\"epoch:\",epoch,\"------------------------------------------------\")\n",
        "            actor.train()\n",
        "\n",
        "            times, losses, rewards, critic_rewards = [], [], [], []\n",
        "            epoch_start = time.time()\n",
        "            start = epoch_start\n",
        "\n",
        "            scheduler = LambdaLR(actor_optim, lr_lambda=lambda f: 0.96 ** epoch)\n",
        "            for batch_idx, batch in enumerate(data_loder):\n",
        "                batch = batch.to(device)\n",
        "                tour_indices, tour_logp = actor(batch,steps*2)\n",
        "\n",
        "                rewar = reward1(batch.x, tour_indices.detach(),n_nodes)\n",
        "                base_reward = rol_baseline.eval(batch,steps)\n",
        "\n",
        "                advantage = (rewar - base_reward)\n",
        "                if not advantage.ne(0).any():\n",
        "                    print(\"advantage==0.\")\n",
        "                advantage = adv_normalize(advantage)\n",
        "                actor_loss = torch.mean(advantage.detach() * tour_logp)\n",
        "\n",
        "                actor_optim.zero_grad()\n",
        "                actor_loss.backward()\n",
        "                #grad_norms = clip_grad_norms(actor_optim.param_groups, 1)\n",
        "                #torch.nn.utils.clip_grad_norm_(actor.parameters(), max_grad_norm)\n",
        "                actor_optim.step()\n",
        "                scheduler.step()\n",
        "                rewards.append(torch.mean(rewar.detach()).item())\n",
        "                losses.append(torch.mean(actor_loss.detach()).item())\n",
        "\n",
        "                step = 200\n",
        "                if (batch_idx + 1) % step == 0:\n",
        "                    end = time.time()\n",
        "                    times.append(end - start)\n",
        "                    start = end\n",
        "\n",
        "                    mean_loss = np.mean(losses[-step:])\n",
        "                    mean_reward = np.mean(rewards[-step:])\n",
        "\n",
        "                    print('  Batch %d/%d, reward: %2.3f, loss: %2.4f, took: %2.4fs' %\n",
        "                          (batch_idx, len(data_loder), mean_reward, mean_loss,\n",
        "                           times[-1]))\n",
        "            rol_baseline.epoch_callback(actor,epoch)\n",
        "\n",
        "            epoch_dir = os.path.join(filepath, '%s' % epoch)\n",
        "            if not os.path.exists(epoch_dir):\n",
        "                os.makedirs(epoch_dir)\n",
        "            save_path = os.path.join(epoch_dir, 'actor.pt')\n",
        "            torch.save(actor.state_dict(), save_path)\n",
        "            cost = rollout(actor, valid_loder, batch_size, steps)\n",
        "            cost = cost.mean()\n",
        "            costs.append(cost.item())\n",
        "            np.savetxt('myarray.txt', costs)\n",
        "\n",
        "            print('Problem:TSP''%s' % n_nodes, '/ Average distance:', cost.item())\n",
        "            print(costs)\n",
        "\n",
        "train()"
      ],
      "metadata": {
        "id": "Cm5aDsnBszxQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}